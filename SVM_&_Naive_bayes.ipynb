{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <<<<<< SVM & Naive bayes >>>>>>"
      ],
      "metadata": {
        "id": "Cs_F3bF11XrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. What is a Support Vector Machine (SVM) ?\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a powerful supervised machine learning algorithm used primarily for **classification** tasks, though it can also be adapted for **regression**. It's especially effective in high-dimensional spaces and for problems where the number of dimensions exceeds the number of samples.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Core Concept\n",
        "\n",
        "SVM aims to find the **optimal hyperplane** that best separates data points of different classes. The goal is to **maximize the margin** between the closest data points of each class‚Äîthese closest points are called **support vectors**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìê How It Works\n",
        "\n",
        "- **Linear SVM**: Finds a straight line (in 2D) or hyperplane (in higher dimensions) that separates classes.\n",
        "- **Margin**: The distance between the hyperplane and the nearest data points from each class.\n",
        "- **Support Vectors**: Data points that lie closest to the decision boundary; they \"support\" the optimal hyperplane.\n",
        "- **Maximizing the Margin**: SVM chooses the hyperplane that has the largest possible margin to improve generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### üå™Ô∏è Non-Linear SVM & Kernel Trick\n",
        "\n",
        "When data isn‚Äôt linearly separable, SVM uses the **kernel trick** to transform the data into a higher-dimensional space where a linear separator **can** be found.\n",
        "\n",
        "Popular kernels:\n",
        "- **Linear kernel**: For linearly separable data\n",
        "- **Polynomial kernel**: Captures polynomial relationships\n",
        "- **Radial Basis Function (RBF)** or **Gaussian kernel**: Great for complex boundaries\n",
        "- **Sigmoid kernel**: Similar to neural networks\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Mathematical Formulation\n",
        "\n",
        "SVM solves the following optimization problem:\n",
        "\n",
        "Minimize:  \n",
        "$$\\frac{1}{2} ||\\mathbf{w}||^2$$  \n",
        "Subject to:  \n",
        "$$y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1$$  \n",
        "for all training samples \\((\\mathbf{x}_i, y_i)\\)\n",
        "\n",
        "Where:\n",
        "- \\(\\mathbf{w}\\) is the weight vector\n",
        "- \\(b\\) is the bias\n",
        "- \\(y_i\\) is the class label (+1 or -1)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Advantages\n",
        "\n",
        "- Effective in high-dimensional spaces\n",
        "- Works well with clear margin of separation\n",
        "- Memory efficient (only support vectors are used)\n",
        "- Versatile with different kernel functions\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Limitations\n",
        "\n",
        "- Not ideal for large datasets (training can be slow)\n",
        "- Performance drops with noisy data or overlapping classes\n",
        "- Choosing the right kernel and tuning parameters (like C and gamma) can be tricky\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5mTlH-4o1nLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2. What is the difference between Hard Margin and Soft Margin SVM\n",
        " The distinction between **Hard Margin** and **Soft Margin** SVM is crucial for understanding how SVM handles real-world data, especially when it's noisy or not perfectly separable.\n",
        "New Section\n",
        "---\n",
        "\n",
        "## üß± Hard Margin vs Soft Margin SVM\n",
        "\n",
        "| Feature              | **Hard Margin SVM**                          | **Soft Margin SVM**                          |\n",
        "|----------------------|----------------------------------------------|----------------------------------------------|\n",
        "| **Assumption**       | Data is perfectly linearly separable         | Data may have overlap or noise               |\n",
        "| **Tolerance for Error** | No misclassification allowed               | Allows some misclassification                |\n",
        "| **Margin**           | Maximizes margin with zero slack             | Maximizes margin with slack variables        |\n",
        "| **Robustness**       | Sensitive to outliers                        | More robust to noisy data                    |\n",
        "| **Use Case**         | Ideal for clean, well-separated data         | Preferred for real-world, messy datasets     |\n",
        "| **Optimization**     | Strict constraints                           | Relaxed constraints with penalty term        |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Hard Margin SVM\n",
        "\n",
        "- Tries to find a hyperplane that **perfectly separates** the classes.\n",
        "- **No data points** are allowed inside the margin.\n",
        "- Works only if the data is **linearly separable**.\n",
        "- Optimization problem:\n",
        "  $$\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2$$  \n",
        "  Subject to:  \n",
        "  $$y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1$$\n",
        "\n",
        "---\n",
        "\n",
        "### üåæ Soft Margin SVM\n",
        "\n",
        "- Introduces **slack variables** \\(\\xi_i\\) to allow some violations of the margin.\n",
        "- Balances between maximizing the margin and minimizing classification error.\n",
        "- Adds a **penalty term** controlled by a regularization parameter \\(C\\):\n",
        "  $$\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{n} \\xi_i$$  \n",
        "  Subject to:  \n",
        "  $$y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i$$  \n",
        "  $$\\xi_i \\geq 0$$\n",
        "\n",
        "- **C** controls the trade-off:\n",
        "  - High \\(C\\): Less tolerance for misclassification (closer to hard margin)\n",
        "  - Low \\(C\\): More tolerance, better generalization\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Intuition\n",
        "\n",
        "- **Hard Margin** is like a strict teacher: no mistakes allowed.\n",
        "- **Soft Margin** is more forgiving: allows a few errors to get a better overall model.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5uTpEfXE2XBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3. What is the mathematical intuition behind SVM ?\n",
        " The intuition behind SVM is rooted in **maximizing the margin** between classes while **minimizing classification error**. Let‚Äôs break it down step by step.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Mathematical Intuition Behind SVM\n",
        "\n",
        "### 1. **Goal: Find the Optimal Hyperplane**\n",
        "\n",
        "In an \\(n\\)-dimensional space, a hyperplane is defined as:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(\\mathbf{w}\\) is the **weight vector** (normal to the hyperplane)\n",
        "- \\(b\\) is the **bias**\n",
        "- \\(\\mathbf{x}\\) is the input vector\n",
        "\n",
        "This hyperplane separates the data into two classes.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Margin: Distance Between Classes**\n",
        "\n",
        "The **margin** is the distance between the hyperplane and the **closest data points** from each class (the support vectors). The goal is to **maximize this margin**.\n",
        "\n",
        "The margin is given by:\n",
        "\n",
        "$$\n",
        "\\text{Margin} = \\frac{2}{||\\mathbf{w}||}\n",
        "$$\n",
        "\n",
        "So, maximizing the margin is equivalent to **minimizing** \\(||\\mathbf{w}||^2\\).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Constraints: Correct Classification**\n",
        "\n",
        "For each training sample \\((\\mathbf{x}_i, y_i)\\), where \\(y_i \\in \\{-1, +1\\}\\), we want:\n",
        "\n",
        "$$\n",
        "y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1\n",
        "$$\n",
        "\n",
        "This ensures that each point is correctly classified and lies **outside the margin**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Optimization Problem (Hard Margin)**\n",
        "\n",
        "Minimize:\n",
        "\n",
        "$$\n",
        "\\frac{1}{2} ||\\mathbf{w}||^2\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "\n",
        "$$\n",
        "y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1\n",
        "$$\n",
        "\n",
        "This is a **convex quadratic optimization problem** with linear constraints.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Soft Margin: Introducing Slack Variables**\n",
        "\n",
        "When data isn‚Äôt perfectly separable, we introduce **slack variables** \\(\\xi_i\\) to allow some violations:\n",
        "\n",
        "Minimize:\n",
        "\n",
        "$$\n",
        "\\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{n} \\xi_i\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "\n",
        "$$\n",
        "y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
        "$$\n",
        "\n",
        "Here, \\(C\\) is a regularization parameter that controls the trade-off between margin size and classification error.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Dual Formulation & Kernel Trick**\n",
        "\n",
        "To handle non-linear boundaries, SVM uses the **dual form** of the optimization problem:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(\\alpha_i\\) are Lagrange multipliers\n",
        "- \\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\) is the **kernel function**\n",
        "\n",
        "This lets SVM operate in a **transformed feature space** without explicitly computing the transformation.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Summary of Intuition\n",
        "\n",
        "- SVM finds the **widest possible margin** between classes.\n",
        "- It uses **support vectors** to define the decision boundary.\n",
        "- It solves a **convex optimization problem** to ensure global optimality.\n",
        "- It can handle **non-linear data** using the **kernel trick**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Dkmd0xWJ4JmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. What is the role of Lagrange Multipliers in SVM\n",
        " Lagrange multipliers are the mathematical bridge that lets SVM elegantly handle **constraints** while optimizing the margin. They‚Äôre the key players in transforming the primal problem into its dual form, which is often easier to solve‚Äîespecially when using kernels.\n",
        "\n",
        "Let‚Äôs unpack their role step by step.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Why Use Lagrange Multipliers in SVM?\n",
        "\n",
        "SVM is a **constrained optimization problem**:\n",
        "\n",
        "- **Objective**: Minimize \\( \\frac{1}{2} ||\\mathbf{w}||^2 \\)\n",
        "- **Constraint**: \\( y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\)\n",
        "\n",
        "To solve this, we use **Lagrange multipliers** to incorporate the constraints into the objective function.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Constructing the Lagrangian\n",
        "\n",
        "We introduce a multiplier \\( \\alpha_i \\geq 0 \\) for each constraint:\n",
        "\n",
        "### **Lagrangian Function:**\n",
        "\n",
        "$$\n",
        "L(\\mathbf{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2} ||\\mathbf{w}||^2 - \\sum_{i=1}^{n} \\alpha_i \\left[ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) - 1 \\right]\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- \\( \\alpha_i \\) penalizes violations of the margin constraint.\n",
        "- If a point is correctly classified and far from the margin, \\( \\alpha_i = 0 \\).\n",
        "- If a point lies on or violates the margin, \\( \\alpha_i > 0 \\).\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Dual Problem: Why It‚Äôs Useful\n",
        "\n",
        "By taking derivatives of \\(L\\) with respect to \\(\\mathbf{w}\\) and \\(b\\), and setting them to zero, we eliminate \\(\\mathbf{w}\\) and \\(b\\) and get the **dual form**:\n",
        "\n",
        "### **Dual Objective:**\n",
        "\n",
        "$$\n",
        "\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i \\cdot \\mathbf{x}_j)\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "\n",
        "- \\( \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\)\n",
        "- \\( \\alpha_i \\geq 0 \\)\n",
        "\n",
        "This dual form:\n",
        "- Depends only on dot products of inputs (enabling the **kernel trick**).\n",
        "- Is a **quadratic programming problem** with linear constraints.\n",
        "- Is often more efficient to solve than the primal.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Intuition Behind \\( \\alpha_i \\)\n",
        "\n",
        "- **Support Vectors**: Only points with \\( \\alpha_i > 0 \\) influence the decision boundary.\n",
        "- **Non-support vectors**: Have \\( \\alpha_i = 0 \\), so they don‚Äôt affect the final model.\n",
        "- The **decision function** becomes:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_i y_i (\\mathbf{x}_i \\cdot \\mathbf{x}) + b\n",
        "$$\n",
        "\n",
        "Only support vectors contribute to this sum.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Summary\n",
        "\n",
        "| Concept | Role of Lagrange Multipliers |\n",
        "|--------|-------------------------------|\n",
        "| Constraints | Encoded via \\( \\alpha_i \\) |\n",
        "| Optimization | Transforms primal to dual |\n",
        "| Support Vectors | Identified by \\( \\alpha_i > 0 \\) |\n",
        "| Kernel Trick | Enabled by dual form |\n",
        "| Decision Boundary | Weighted sum of support vectors |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WpOH77UT5V84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5.What are Support Vectors in SVM\n",
        "Support vectors are the **VIPs** of the SVM world‚Äîthey‚Äôre the data points that **define the decision boundary**. Without them, the SVM wouldn‚Äôt know where to draw the line (literally). Let‚Äôs break down their role and intuition.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What Are Support Vectors?\n",
        "\n",
        "Support vectors are the **data points closest to the separating hyperplane**. They lie **on the margin** or **violate it slightly** (in soft-margin SVM). These are the only points that directly influence the position and orientation of the hyperplane.\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Geometric Intuition\n",
        "\n",
        "Imagine a 2D classification problem. The SVM finds a line that separates the two classes with the **maximum margin**. The margin is the space between two parallel lines that touch the closest points from each class.\n",
        "\n",
        "Those closest points are the **support vectors**.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Mathematical Role\n",
        "\n",
        "In the dual formulation of SVM, the decision function is:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_i y_i (\\mathbf{x}_i \\cdot \\mathbf{x}) + b\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- \\( \\alpha_i > 0 \\) ‚Üí \\( \\mathbf{x}_i \\) is a **support vector**\n",
        "- \\( \\alpha_i = 0 \\) ‚Üí \\( \\mathbf{x}_i \\) does **not** affect the decision boundary\n",
        "\n",
        "So, only support vectors contribute to the final classifier.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Are They Important?\n",
        "\n",
        "- **Efficiency**: The model depends only on a subset of the data.\n",
        "- **Robustness**: Removing non-support vectors doesn‚Äôt change the decision boundary.\n",
        "- **Interpretability**: They help us understand which data points are critical for classification.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example in Practice\n",
        "\n",
        "Let‚Äôs say you train an SVM on 1000 data points. After training, only 30 have \\( \\alpha_i > 0 \\). These 30 are the **support vectors**‚Äîthe rest are irrelevant to the final decision boundary.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Visualization Summary\n",
        "\n",
        "| Feature | Description |\n",
        "|--------|-------------|\n",
        "| Location | On or near the margin |\n",
        "| Influence | Define the hyperplane |\n",
        "| Count | Usually a small subset of data |\n",
        "| Identified by | \\( \\alpha_i > 0 \\) in dual form |\n",
        "| Contribution | Directly affect decision function |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jomoz9nr6i7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6. What is a Support Vector Classifier (SVC)\n",
        "A **Support Vector Classifier (SVC)** is the practical implementation of the Support Vector Machine (SVM) algorithm for classification tasks. It‚Äôs the tool that takes all the beautiful theory‚Äîhyperplanes, margins, kernels‚Äîand turns it into a working model that can classify data.\n",
        "\n",
        "Let‚Äôs break it down clearly:\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What Is SVC?\n",
        "\n",
        "SVC stands for **Support Vector Classifier**, and it‚Äôs a class in libraries like **scikit-learn** that implements the SVM algorithm for **binary or multi-class classification**.\n",
        "\n",
        "It finds the **optimal decision boundary** (hyperplane) that separates classes with the **maximum margin**, using only the **support vectors**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß How SVC Works\n",
        "\n",
        "1. **Input**: Training data \\((\\mathbf{x}_i, y_i)\\), where \\(y_i \\in \\{-1, +1\\}\\) or more classes.\n",
        "2. **Objective**: Find a hyperplane that separates classes with the largest margin.\n",
        "3. **Optimization**: Uses Lagrange multipliers to solve a constrained quadratic optimization problem.\n",
        "4. **Kernel Trick**: Can use kernels to handle non-linear boundaries.\n",
        "5. **Output**: A decision function that classifies new data points.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ SVC in Python (scikit-learn)\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create the model\n",
        "model = SVC(kernel='linear', C=1)\n",
        "\n",
        "# Fit to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Support vectors\n",
        "support_vectors = model.support_vectors_\n",
        "```\n",
        "\n",
        "You can choose different kernels:\n",
        "- `'linear'` for linear separation\n",
        "- `'rbf'` for radial basis function (non-linear)\n",
        "- `'poly'` for polynomial boundaries\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Key Features of SVC\n",
        "\n",
        "| Feature | Description |\n",
        "|--------|-------------|\n",
        "| **Margin Maximization** | Separates classes with widest possible margin |\n",
        "| **Support Vectors** | Only critical points define the boundary |\n",
        "| **Kernel Trick** | Projects data into higher dimensions for non-linear separation |\n",
        "| **Regularization (C)** | Controls trade-off between margin size and misclassification |\n",
        "| **Multi-class Support** | Uses one-vs-one or one-vs-rest strategies |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Intuition Recap\n",
        "\n",
        "SVC is like a **smart boundary drawer**‚Äîit doesn‚Äôt just separate classes, it does so in the most confident way possible, relying only on the most informative data points.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yaTBK5Hb7SY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7.What is a Support Vector Regressor (SVR)\n",
        "---\n",
        "\n",
        "## üß† What Is SVR?\n",
        "\n",
        "**Support Vector Regression (SVR)** is a type of Support Vector Machine used for **predicting continuous values**. Instead of finding a hyperplane that separates classes, SVR finds a function that **approximates the data within a margin of tolerance**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Intuition Behind SVR\n",
        "\n",
        "Imagine fitting a **flat tube** (margin) around your data points. SVR tries to find a function \\( f(x) \\) such that:\n",
        "\n",
        "- Most data points lie **within the tube** (i.e., within a tolerance \\( \\varepsilon \\))\n",
        "- Points outside the tube are penalized\n",
        "- Only the points **outside the tube** become **support vectors**\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Mathematical Formulation\n",
        "\n",
        "SVR solves the following optimization problem:\n",
        "\n",
        "### **Objective:**\n",
        "Minimize:\n",
        "\n",
        "$$\n",
        "\\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i - (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\leq \\varepsilon + \\xi_i \\\\\n",
        "(\\mathbf{w} \\cdot \\mathbf{x}_i + b) - y_i \\leq \\varepsilon + \\xi_i^* \\\\\n",
        "\\xi_i, \\xi_i^* \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( \\varepsilon \\) is the **tube width** (tolerance)\n",
        "- \\( \\xi_i, \\xi_i^* \\) are **slack variables** for points outside the tube\n",
        "- \\( C \\) controls the trade-off between flatness and tolerance violations\n",
        "\n",
        "---\n",
        "\n",
        "## üîß SVR in Python (scikit-learn)\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Create the model\n",
        "model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "\n",
        "# Fit to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Support vectors\n",
        "support_vectors = model.support_\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Key Features of SVR\n",
        "\n",
        "| Feature | Description |\n",
        "|--------|-------------|\n",
        "| **Epsilon-tube** | Defines tolerance for prediction error |\n",
        "| **Support Vectors** | Points outside the tube that influence the model |\n",
        "| **Kernel Trick** | Enables non-linear regression |\n",
        "| **Regularization (C)** | Controls penalty for errors outside the tube |\n",
        "| **Dual Formulation** | Solves using Lagrange multipliers |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Intuition Recap\n",
        "\n",
        "SVR doesn‚Äôt try to predict every point perfectly‚Äîit tries to **stay within a reasonable error margin**. It‚Äôs robust, efficient, and ideal when you care more about general trends than exact fits.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Z15eL3-P7trr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8. What is the Kernel Trick in SVM\n",
        " the **kernel trick**‚ÄîSVM‚Äôs secret weapon for handling complex, non-linear data without breaking a sweat. It‚Äôs one of the most elegant ideas in machine learning, allowing SVM to operate in **higher-dimensional spaces** without explicitly computing those transformations.\n",
        "\n",
        "Let‚Äôs unpack it step by step.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What Is the Kernel Trick?\n",
        "\n",
        "The **kernel trick** allows SVM to learn **non-linear decision boundaries** by implicitly mapping data into a **higher-dimensional space**, where a linear separator *might* exist.\n",
        "\n",
        "Instead of transforming the data manually, the kernel trick computes the **dot product in the transformed space** using a **kernel function**‚Äîwithout ever performing the transformation.\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Why It Works\n",
        "\n",
        "In the dual form of SVM, the decision function depends only on **dot products** between data points:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\n",
        "$$\n",
        "\n",
        "Here, \\(K(\\mathbf{x}_i, \\mathbf{x})\\) is the **kernel function** that computes the dot product in the transformed space.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Common Kernel Functions\n",
        "\n",
        "| Kernel | Formula | Use Case |\n",
        "|--------|--------|----------|\n",
        "| **Linear** | \\(K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x} \\cdot \\mathbf{z}\\) | When data is linearly separable |\n",
        "| **Polynomial** | \\(K(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x} \\cdot \\mathbf{z} + c)^d\\) | Captures polynomial relationships |\n",
        "| **RBF (Gaussian)** | \\(K(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma ||\\mathbf{x} - \\mathbf{z}||^2)\\) | Handles complex, non-linear boundaries |\n",
        "| **Sigmoid** | \\(K(\\mathbf{x}, \\mathbf{z}) = \\tanh(\\alpha \\mathbf{x} \\cdot \\mathbf{z} + c)\\) | Inspired by neural networks |\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example in Python (scikit-learn)\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# RBF kernel for non-linear classification\n",
        "model = SVC(kernel='rbf', gamma=0.5, C=1)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Intuition Recap\n",
        "\n",
        "- The kernel trick lets SVM **act as if** it‚Äôs working in a high-dimensional space.\n",
        "- It avoids the **computational cost** of explicitly transforming data.\n",
        "- It enables SVM to learn **non-linear patterns** using **linear methods** in transformed space.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Visual Analogy\n",
        "\n",
        "Imagine trying to separate two intertwined spirals in 2D‚Äîit‚Äôs impossible with a straight line. But if you could lift the data into 3D, maybe a plane could separate them. The kernel trick lets you do that lifting **mathematically**, without ever touching the third dimension.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "HrNANkDD89s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9.Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "Here‚Äôs a clear comparison of the **Linear**, **Polynomial**, and **RBF (Radial Basis Function)** kernels used in Support Vector Machines (SVM). Each kernel transforms the input data differently, enabling SVM to find decision boundaries that best separate the classes.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Kernel Comparison Table\n",
        "\n",
        "| Kernel Type     | Formula                                | Use Case                            | Pros                                         | Cons                                         |\n",
        "|----------------|-----------------------------------------|-------------------------------------|----------------------------------------------|----------------------------------------------|\n",
        "| **Linear**      | \\( K(x, y) = x \\cdot y \\)              | Linearly separable data             | Simple, fast, interpretable                  | Poor performance on non-linear data          |\n",
        "| **Polynomial**  | \\( K(x, y) = (x \\cdot y + c)^d \\)      | Data with polynomial relationships  | Captures complex patterns                    | Sensitive to degree \\(d\\), slower to train   |\n",
        "| **RBF (Gaussian)** | \\( K(x, y) = \\exp(-\\gamma ||x - y||^2) \\) | Non-linear, unknown structure       | Highly flexible, handles complex boundaries  | Requires tuning of \\(\\gamma\\), less interpretable |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Intuition Behind Each Kernel\n",
        "\n",
        "- **Linear Kernel**: Think of a straight line (or hyperplane) slicing through the data. Great when the data is already well-separated.\n",
        "- **Polynomial Kernel**: Adds curvature to the decision boundary. The higher the degree, the more complex the boundary.\n",
        "- **RBF Kernel**: Creates smooth, flexible boundaries by measuring similarity using distance. It‚Äôs like placing a bell-shaped curve around each data point.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Performance Insights\n",
        "\n",
        "- **Linear**: Fastest to train and predict. Ideal for high-dimensional sparse data (e.g., text classification).\n",
        "- **Polynomial**: Can model intricate relationships but may overfit if degree is too high.\n",
        "- **RBF**: Most commonly used. Performs well on a wide range of problems but requires careful tuning of parameters like \\( \\gamma \\) and \\( C \\).\n",
        "\n",
        "> ‚ÄúLinear kernels are suited for linearly separable data, while polynomial and RBF kernels offer greater flexibility for complex datasets‚Äù ‚Äî [Demystifying SVM Kernels](https://mldemystified.com/posts/basics-of-ml/support-vector-machines/svm-kernel-machines/)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3im640-f_KHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #Q10. What is the effect of the C parameter in SVM\n",
        " The **C parameter** in Support Vector Machines (SVM) is a **regularization parameter** that controls the **trade-off between maximizing the margin and minimizing classification error**. It plays a pivotal role in shaping the decision boundary and determining how strictly the model fits the training data.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Intuition Behind C\n",
        "\n",
        "- **Small C** ‚Üí More tolerance for misclassifications  \n",
        "- **Large C** ‚Üí Less tolerance for misclassifications\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Mathematical Role\n",
        "\n",
        "In soft-margin SVM, the optimization objective is:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{n} \\xi_i\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( ||\\mathbf{w}||^2 \\) controls margin size\n",
        "- \\( \\xi_i \\) are slack variables for misclassified points\n",
        "- \\( C \\) penalizes misclassifications\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Effects of C\n",
        "\n",
        "| C Value | Margin | Misclassification | Risk |\n",
        "|--------|--------|-------------------|------|\n",
        "| **Low C** | Wider | More allowed | Underfitting |\n",
        "| **High C** | Narrower | Less allowed | Overfitting |\n",
        "\n",
        "- A **low C** encourages a **simpler model** with a wider margin, even if it misclassifies some training points.\n",
        "- A **high C** forces the model to **fit the training data tightly**, possibly at the cost of generalization.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Practical Example\n",
        "\n",
        "In noisy datasets:\n",
        "- **Low C** helps ignore outliers and noise.\n",
        "- **High C** tries to classify every point correctly, which can lead to overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "- **C is a regularization knob**: it tunes how much the model cares about training accuracy vs generalization.\n",
        "- Choosing the right C is crucial for balancing bias and variance.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3rm3H30_kQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10. What is the effect of the C parameter in SVM\n",
        "The **C parameter** in Support Vector Machines (SVM) is a **regularization hyperparameter** that controls the **trade-off between maximizing the margin and minimizing classification error**. Its value directly influences the shape and complexity of the decision boundary.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è How C Affects SVM\n",
        "\n",
        "### üîπ **Low C Value**  \n",
        "- **Effect**: Allows more misclassifications  \n",
        "- **Margin**: Wider  \n",
        "- **Model Behavior**: Focuses on generalization  \n",
        "- **Risk**: May underfit the data  \n",
        "- **Best For**: Noisy datasets or when overfitting is a concern\n",
        "\n",
        "> A smaller value of C allows for a larger margin, potentially leading to more misclassifications on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **High C Value**  \n",
        "- **Effect**: Penalizes misclassifications heavily  \n",
        "- **Margin**: Narrower  \n",
        "- **Model Behavior**: Tries to classify all training points correctly  \n",
        "- **Risk**: May overfit the data  \n",
        "- **Best For**: Clean, well-separated datasets\n",
        "\n",
        "> A larger value of C emphasizes minimizing the training error, potentially resulting in a narrower margin.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary Table\n",
        "\n",
        "| C Value | Margin Width | Misclassification Tolerance | Risk |\n",
        "|--------|---------------|-----------------------------|------|\n",
        "| Low    | Wide          | High                        | Underfitting |\n",
        "| High   | Narrow        | Low                         | Overfitting |\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Visual Insight\n",
        "\n",
        "Imagine a decision boundary:\n",
        "- With **low C**, it‚Äôs smooth and forgiving‚Äîsome points may be misclassified to keep the margin wide.\n",
        "- With **high C**, it bends tightly around every point‚Äîtrying to get everything right, even if the margin shrinks.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TVjuK75iCg4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q11. What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "The **gamma parameter** in an SVM with an **RBF (Radial Basis Function) kernel** controls how far the influence of a single training example reaches. It‚Äôs a key hyperparameter that shapes the **complexity of the decision boundary** and directly affects model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Intuition Behind Gamma\n",
        "\n",
        "- **Low gamma** ‚Üí Each training point has a **wide influence**  \n",
        "- **High gamma** ‚Üí Each training point has a **narrow influence**\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Mathematical Role\n",
        "\n",
        "The RBF kernel is defined as:\n",
        "\n",
        "$$\n",
        "K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)\n",
        "$$\n",
        "\n",
        "Here, \\( \\gamma \\) determines how quickly the similarity between points decays with distance.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Effects of Gamma\n",
        "\n",
        "| Gamma Value | Influence Radius | Decision Boundary | Risk |\n",
        "|-------------|------------------|-------------------|------|\n",
        "| **Low**     | Large            | Smooth, simple    | Underfitting |\n",
        "| **High**    | Small            | Complex, wiggly   | Overfitting |\n",
        "\n",
        "- **Low gamma**: The model considers distant points similar, resulting in a smoother boundary.\n",
        "- **High gamma**: The model focuses only on nearby points, creating tight, intricate boundaries that may capture noise.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Gamma vs C Interaction\n",
        "\n",
        "- **High gamma + high C** ‚Üí Very tight fit, likely overfitting\n",
        "- **Low gamma + low C** ‚Üí Loose fit, possibly underfitting\n",
        "- **Balanced gamma and C** ‚Üí Optimal generalization\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Tuning Gamma\n",
        "\n",
        "- Use **grid search** or **random search** with cross-validation.\n",
        "- Try values like \\( \\gamma \\in \\{2^{-15}, 2^{-14}, \\dots, 2^3\\} \\).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TdadF_BjDaFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"\n",
        "The **Na√Øve Bayes classifier** is a **probabilistic machine learning algorithm** based on **Bayes‚Äô Theorem**, used primarily for classification tasks like spam detection, sentiment analysis, and document categorization.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What It Does\n",
        "\n",
        "Na√Øve Bayes calculates the probability that a data point belongs to a particular class, given its features. It uses:\n",
        "\n",
        "$$\n",
        "P(C \\mid X) = \\frac{P(X \\mid C) \\cdot P(C)}{P(X)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( P(C \\mid X) \\) is the **posterior probability** of class \\( C \\) given features \\( X \\)\n",
        "- \\( P(X \\mid C) \\) is the **likelihood**\n",
        "- \\( P(C) \\) is the **prior probability** of class \\( C \\)\n",
        "- \\( P(X) \\) is the **evidence** (often ignored in classification)\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Why It's Called \"Na√Øve\"\n",
        "\n",
        "It‚Äôs called **‚ÄúNa√Øve‚Äù** because it makes a **strong assumption**:  \n",
        "> All features are **conditionally independent** given the class label.\n",
        "\n",
        "This means the presence or absence of one feature is assumed to have **no effect** on the presence or absence of another feature‚Äîan assumption that‚Äôs rarely true in real-world data.\n",
        "\n",
        "Despite this simplification, Na√Øve Bayes often performs surprisingly well, especially in high-dimensional spaces like text classification.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Key Advantages\n",
        "\n",
        "- **Fast and efficient** for large datasets\n",
        "- **Works well with high-dimensional data**\n",
        "- **Simple to implement and interpret**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SC5yy1wFEdOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q13. What is Bayes‚Äô Theorem.\n",
        "Bayes‚Äô Theorem is a fundamental concept in probability theory that describes how to **update the probability of a hypothesis** based on new evidence. It‚Äôs the backbone of Bayesian inference and widely used in fields like machine learning, medicine, and decision-making under uncertainty.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò Bayes‚Äô Theorem Formula\n",
        "\n",
        "For two events \\( A \\) and \\( B \\), the theorem is expressed as:\n",
        "\n",
        "$$\n",
        "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( P(A \\mid B) \\): Posterior probability ‚Äî probability of event \\( A \\) given \\( B \\)\n",
        "- \\( P(B \\mid A) \\): Likelihood ‚Äî probability of observing \\( B \\) given \\( A \\)\n",
        "- \\( P(A) \\): Prior probability ‚Äî initial belief about \\( A \\)\n",
        "- \\( P(B) \\): Evidence ‚Äî total probability of observing \\( B \\)\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Intuition\n",
        "\n",
        "Imagine you're trying to diagnose a disease:\n",
        "- **Prior**: How common is the disease in the population?\n",
        "- **Likelihood**: How likely is a positive test result if someone has the disease?\n",
        "- **Evidence**: How likely is a positive test result overall?\n",
        "- **Posterior**: Given a positive test, how likely is it that the person actually has the disease?\n",
        "\n",
        "Bayes‚Äô Theorem helps you **revise your belief** about the disease status based on the test result.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Applications\n",
        "\n",
        "- **Spam filtering**: Is an email spam given certain keywords?\n",
        "- **Medical diagnosis**: What‚Äôs the probability of a disease given symptoms?\n",
        "- **Machine learning**: Used in classifiers like Na√Øve Bayes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0hVE1jXIFTTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q14. Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes.\n",
        " The three main variants of the **Na√Øve Bayes classifier**‚Äî**Gaussian**, **Multinomial**, and **Bernoulli**‚Äîare tailored to different types of data distributions and feature representations. Here's a breakdown to help you choose the right one:\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Comparison of Na√Øve Bayes Variants\n",
        "\n",
        "| Variant               | Best For                     | Feature Type         | Distribution Assumption         | Example Use Case                     |\n",
        "|----------------------|------------------------------|----------------------|----------------------------------|--------------------------------------|\n",
        "| **Gaussian Na√Øve Bayes** | Continuous data               | Real-valued features | Normal (Gaussian) distribution   | Predicting age, income, or height    |\n",
        "| **Multinomial Na√Øve Bayes** | Count-based text data         | Discrete counts      | Multinomial distribution         | Spam detection, document classification |\n",
        "| **Bernoulli Na√Øve Bayes** | Binary/Boolean features        | 0/1 indicators       | Bernoulli distribution           | Sentiment analysis, keyword presence |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Details\n",
        "\n",
        "### 1. **Gaussian Na√Øve Bayes**\n",
        "- Assumes features follow a **normal distribution**.\n",
        "- Each feature is modeled using its **mean and variance**.\n",
        "- Common in **continuous datasets** like sensor readings or biometric data.\n",
        "\n",
        "### 2. **Multinomial Na√Øve Bayes**\n",
        "- Assumes features are **counts or frequencies**.\n",
        "- Often used in **text classification**, where features are word counts.\n",
        "- Sensitive to **feature magnitude**, not just presence.\n",
        "\n",
        "### 3. **Bernoulli Na√Øve Bayes**\n",
        "- Assumes features are **binary** (e.g., word present or not).\n",
        "- Suitable for **sparse binary data**.\n",
        "- Less sensitive to word frequency, focuses on **presence/absence**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Quick Tip\n",
        "\n",
        "- For **text data**:\n",
        "  - Use **Multinomial NB** if you care about word frequency.\n",
        "  - Use **Bernoulli NB** if you only care whether a word appears.\n",
        "- For **numerical data**:\n",
        "  - Use **Gaussian NB** if features are continuous and roughly bell-shaped.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cg4_qHMOGRnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q15. When should you use Gaussian Na√Øve Bayes over other variants\n",
        "You should use **Gaussian Na√Øve Bayes** when your dataset contains **continuous features** that are approximately **normally distributed**. It‚Äôs especially effective when the assumption of Gaussian (bell-shaped) distribution holds true for each feature within each class.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ When to Use Gaussian Na√Øve Bayes\n",
        "\n",
        "### 1. **Continuous Data**\n",
        "- Ideal for features like **age**, **income**, **height**, **temperature**, etc.\n",
        "- Each feature is modeled using its **mean and standard deviation** per class.\n",
        "\n",
        "### 2. **Features Follow a Normal Distribution**\n",
        "- Works best when the data within each class resembles a **Gaussian curve**.\n",
        "- You can check this with histograms or statistical tests like the Shapiro-Wilk test.\n",
        "\n",
        "### 3. **Fast and Interpretable Models**\n",
        "- Great for **small datasets** or when you need a **quick baseline model**.\n",
        "- Easy to implement and interpret, especially in real-time systems.\n",
        "\n",
        "### 4. **Low Dimensionality**\n",
        "- Performs well when the number of features is moderate and not highly correlated.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Example Use Cases\n",
        "\n",
        "- **Medical diagnosis**: Predicting disease based on continuous biomarkers like blood pressure or cholesterol levels.\n",
        "- **Sensor data classification**: Classifying environmental conditions from temperature, humidity, etc.\n",
        "- **Finance**: Predicting credit risk based on income, debt ratio, and age.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "T_jKSo0gG9fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q16. What are the key assumptions made by Na√Øve Bayes\n",
        "Here are the **key assumptions** made by the **Na√Øve Bayes classifier**, which give it both its simplicity and its ‚Äúna√Øve‚Äù label:\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Core Assumptions of Na√Øve Bayes\n",
        "\n",
        "### 1. **Conditional Independence of Features**\n",
        "- The most fundamental assumption:\n",
        "  > Given the class label, all features are **independent** of each other.\n",
        "- Mathematically:\n",
        "  $$\n",
        "  P(x_1, x_2, ..., x_n \\mid y) = \\prod_{i=1}^{n} P(x_i \\mid y)\n",
        "  $$\n",
        "- This is rarely true in real-world data, but it simplifies computation dramatically.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Equal Contribution of Features**\n",
        "- Every feature contributes **equally** and **independently** to the final classification.\n",
        "- No feature is considered more important unless explicitly modeled.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Correct Distributional Assumption (Variant-Specific)**\n",
        "Depending on the variant used:\n",
        "| Variant                  | Assumes Features Follow...       | Suitable For...              |\n",
        "|--------------------------|----------------------------------|------------------------------|\n",
        "| **Gaussian Na√Øve Bayes** | Normal (Gaussian) distribution   | Continuous data              |\n",
        "| **Multinomial Na√Øve Bayes** | Multinomial distribution         | Count data (e.g., word frequencies) |\n",
        "| **Bernoulli Na√Øve Bayes** | Bernoulli distribution (binary) | Binary/Boolean features      |\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **No Interaction Between Features**\n",
        "- The model does **not account for correlations** or interactions between features.\n",
        "- For example, in text classification, it treats ‚Äúnot good‚Äù the same as ‚Äúgood not,‚Äù ignoring word order and context.\n",
        "\n",
        "---\n",
        "\n",
        "Despite these assumptions being strong and often unrealistic, Na√Øve Bayes performs surprisingly well in many domains like spam filtering, sentiment analysis, and document classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "AFFYHjB2NTm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q17. What are the advantages and disadvantages of Na√Øve Bayes\n",
        "Here‚Äôs a clear breakdown of the **advantages and disadvantages** of the Na√Øve Bayes algorithm, based on both theory and practical insights:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Advantages of Na√Øve Bayes\n",
        "\n",
        "### 1. **Simplicity and Ease of Implementation**\n",
        "- Based on Bayes‚Äô Theorem, it's mathematically straightforward.\n",
        "- Easy to code and interpret, making it ideal for beginners.\n",
        "\n",
        "### 2. **Fast and Scalable**\n",
        "- Training and prediction are extremely fast.\n",
        "- Suitable for real-time applications and large datasets.\n",
        "\n",
        "### 3. **Performs Well with High-Dimensional Data**\n",
        "- Especially effective in text classification tasks like spam detection or sentiment analysis.\n",
        "- Handles thousands of features (e.g., words) efficiently.\n",
        "\n",
        "### 4. **Works Well with Small Datasets**\n",
        "- Doesn‚Äôt require large amounts of training data to make accurate predictions.\n",
        "\n",
        "### 5. **Robust to Irrelevant Features**\n",
        "- Because it treats features independently, irrelevant ones don‚Äôt heavily impact performance.\n",
        "\n",
        "### 6. **Handles Missing Data Gracefully**\n",
        "- Can ignore missing features during prediction without major loss in accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå Disadvantages of Na√Øve Bayes\n",
        "\n",
        "### 1. **Strong Independence Assumption**\n",
        "- Assumes all features are conditionally independent given the class label.\n",
        "- This rarely holds true in real-world data, leading to suboptimal predictions.\n",
        "\n",
        "### 2. **Poor Performance with Correlated Features**\n",
        "- If features are correlated (e.g., ‚Äúheight‚Äù and ‚Äúweight‚Äù), Na√Øve Bayes may misclassify.\n",
        "\n",
        "### 3. **Zero-Frequency Problem**\n",
        "- If a feature value wasn‚Äôt seen in training for a given class, the probability becomes zero.\n",
        "- Can be mitigated using techniques like **Laplace smoothing**.\n",
        "\n",
        "### 4. **Limited Expressiveness**\n",
        "- Doesn‚Äôt capture complex relationships or interactions between features.\n",
        "- Not ideal for tasks requiring deep feature understanding (e.g., image recognition).\n",
        "\n",
        "### 5. **Distribution Assumption Sensitivity**\n",
        "- Each variant (Gaussian, Multinomial, Bernoulli) assumes a specific distribution.\n",
        "- Misalignment between data and assumed distribution can hurt performance.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "snunUqITO2RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q18.Why is Na√Øve Bayes a good choice for text classification\n",
        "Na√Øve Bayes is a **popular and effective choice for text classification** due to its simplicity, speed, and surprisingly strong performance‚Äîeven when its assumptions are violated. Here's why it stands out:\n",
        "\n",
        "---\n",
        "\n",
        "## üåü Why Na√Øve Bayes Works Well for Text Classification\n",
        "\n",
        "### 1. **Handles High-Dimensional Data Efficiently**\n",
        "- Text data often involves thousands of features (words).\n",
        "- Na√Øve Bayes thrives in such environments because it treats each word independently, reducing computational complexity.\n",
        "\n",
        "### 2. **Fast Training and Prediction**\n",
        "- It requires minimal training time and can make predictions quickly.\n",
        "- Ideal for real-time applications like spam filtering or news categorization.\n",
        "\n",
        "### 3. **Works Well with Sparse Data**\n",
        "- In text classification, most documents contain only a small subset of the total vocabulary.\n",
        "- Na√Øve Bayes handles this sparsity naturally using word presence or frequency.\n",
        "\n",
        "### 4. **Effective with Small Datasets**\n",
        "- Even with limited labeled data, it can produce reliable results.\n",
        "- This makes it useful in domains where annotated data is scarce.\n",
        "\n",
        "### 5. **Multinomial Variant Suits Text Well**\n",
        "- The **Multinomial Na√Øve Bayes** model is designed for discrete features like word counts.\n",
        "- It captures the frequency of words in documents, which is crucial for tasks like sentiment analysis or topic classification.\n",
        "\n",
        "### 6. **Minimal Feature Engineering Required**\n",
        "- You don‚Äôt need complex preprocessing or feature extraction.\n",
        "- Simple representations like bag-of-words or TF-IDF work well with Na√Øve Bayes.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Real-World Applications\n",
        "- **Spam Detection**: Classifies emails based on word patterns.\n",
        "- **Sentiment Analysis**: Determines positive or negative tone in reviews.\n",
        "- **News Categorization**: Assigns articles to topics like politics, sports, or tech.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "HfrM0lhtQVSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q19.Compare SVM and Na√Øve Bayes for classification tasks\n",
        "Here‚Äôs a comprehensive comparison of **Support Vector Machines (SVM)** and **Na√Øve Bayes (NB)** for classification tasks, highlighting their strengths, weaknesses, and ideal use cases:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öîÔ∏è SVM vs Na√Øve Bayes: Side-by-Side Comparison\n",
        "\n",
        "| Feature                     | **Support Vector Machine (SVM)**                             | **Na√Øve Bayes (NB)**                                      |\n",
        "|----------------------------|---------------------------------------------------------------|-----------------------------------------------------------|\n",
        "| **Type of Model**          | Discriminative (focuses on decision boundary)                | Generative (models class probabilities)                   |\n",
        "| **Core Idea**              | Finds optimal hyperplane with maximum margin                 | Applies Bayes‚Äô theorem with independence assumption       |\n",
        "| **Feature Assumption**     | No assumption; can model complex relationships               | Assumes features are conditionally independent            |\n",
        "| **Data Type Suitability**  | Works well with continuous and sparse data                   | Best with categorical or count-based features             |\n",
        "| **Performance on Text**    | High accuracy, especially with kernels                       | Fast and effective, especially with Multinomial NB        |\n",
        "| **Speed & Efficiency**     | Slower training, especially on large datasets                | Extremely fast training and prediction                    |\n",
        "| **Scalability**            | Less scalable for very large datasets                        | Highly scalable, even with high-dimensional data          |\n",
        "| **Interpretability**       | Harder to interpret; no probabilistic output                 | Easy to interpret; provides class probabilities           |\n",
        "| **Robustness to Noise**    | Robust with proper tuning                                    | Sensitive to feature correlations and zero-frequency      |\n",
        "| **Hyperparameter Tuning**  | Requires tuning (C, gamma, kernel)                           | Minimal tuning; smoothing may be needed                   |\n",
        "| **Use Cases**              | Image classification, bioinformatics, sentiment analysis     | Spam detection, document classification, medical diagnosis |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† When to Use Which?\n",
        "\n",
        "### Use **SVM** when:\n",
        "- You need **high accuracy** and can afford longer training time.\n",
        "- Your data is **not linearly separable** and you want to use the **kernel trick**.\n",
        "- You‚Äôre working with **complex feature interactions** or **continuous data**.\n",
        "\n",
        "### Use **Na√Øve Bayes** when:\n",
        "- You need a **fast, simple model** that works well with **text or categorical data**.\n",
        "- Your features are **mostly independent** or you‚Äôre okay with the assumption.\n",
        "- You‚Äôre dealing with **high-dimensional sparse data** (e.g., bag-of-words).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uaKWfVJ2TvoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "Laplace Smoothing is a crucial technique used in **Na√Øve Bayes classifiers** to address the **zero probability problem**, which can severely impact classification performance.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Why Laplace Smoothing Is Needed\n",
        "\n",
        "In Na√Øve Bayes, we calculate the probability of a feature given a class using:\n",
        "\n",
        "\\[\n",
        "P(x_i \\mid y) = \\frac{\\text{count}(x_i, y)}{\\text{count}(y)}\n",
        "\\]\n",
        "\n",
        "But if a feature \\( x_i \\) never appears with class \\( y \\) in the training data, this probability becomes **zero**, which causes the entire product of probabilities to collapse to zero‚Äîleading to incorrect predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÇ How Laplace Smoothing Works\n",
        "\n",
        "Laplace Smoothing modifies the probability formula by adding a small constant (usually 1) to all counts:\n",
        "\n",
        "\\[\n",
        "P(x_i \\mid y) = \\frac{\\text{count}(x_i, y) + \\alpha}{\\text{count}(y) + \\alpha \\cdot N}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\alpha \\) is the smoothing parameter (commonly set to 1)\n",
        "- \\( N \\) is the number of possible feature values (e.g., vocabulary size in text classification)\n",
        "\n",
        "This ensures that **no probability is ever zero**, even for unseen features.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Benefits of Laplace Smoothing\n",
        "\n",
        "- **Prevents zero probabilities** for unseen feature-class combinations\n",
        "- **Improves generalization** on test data\n",
        "- **Simple to implement** and tune (just one parameter: Œ±)\n",
        "- Especially useful in **text classification** tasks with sparse data\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "orAlYEQyUkK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#         <<<<< Practical >>>>>"
      ],
      "metadata": {
        "id": "1H4u05PJVQN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## üå∏ SVM on Iris Dataset: Python Code\n",
        "\n",
        "```python\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# üåº Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# ‚úÇÔ∏è Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚öôÔ∏è Create and train the SVM classifier\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# üîç Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# üìä Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM classifier on Iris dataset: {accuracy:.2f}\")\n",
        "\n",
        "# üßæ Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Notes\n",
        "\n",
        "- `kernel='linear'` uses a linear decision boundary. You can try `'rbf'`, `'poly'`, or `'sigmoid'` for nonlinear classification.\n",
        "- `C=1.0` is the regularization parameter. Lower values increase regularization.\n",
        "- `classification_report` gives precision, recall, and F1-score for each class.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "P1M8Vg7vVfKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies\n",
        "   a Python program that trains **two SVM classifiers**‚Äîone with a **Linear kernel** and one with an **RBF (Radial Basis Function) kernel**‚Äîon the **Wine dataset**, and compares their accuracies.\n",
        "\n",
        "---\n",
        "\n",
        "## üç∑ SVM Comparison on Wine Dataset\n",
        "\n",
        "```python\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# üçá Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# ‚öñÔ∏è Standardize features for better SVM performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ‚úÇÔ∏è Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# üîß Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# üîß Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# üìä Compare accuracies\n",
        "print(f\"Accuracy with Linear Kernel: {acc_linear:.2f}\")\n",
        "print(f\"Accuracy with RBF Kernel:    {acc_rbf:.2f}\")\n",
        "\n",
        "# üèÜ Print which performed better\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"üîπ Linear kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"üîπ RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"üîπ Both kernels performed equally well.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Standardization Matters\n",
        "SVMs are sensitive to feature scales. The `StandardScaler` ensures all features have mean 0 and variance 1, which is especially important for RBF kernels.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iXj8CzdeWxm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q23.Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)\n",
        "\n",
        "---\n",
        "\n",
        "## üè† SVR on Housing Data with MSE Evaluation\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# üè° Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# ‚öñÔ∏è Standardize features\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
        "\n",
        "# ‚úÇÔ∏è Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# üîß Train SVR model\n",
        "svr_model = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# üîç Predict and evaluate\n",
        "y_pred_scaled = svr_model.predict(X_test)\n",
        "\n",
        "# üîÑ Inverse transform predictions and targets to original scale\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "y_true = scaler_y.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "# üìâ Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(f\"Mean Squared Error (SVR on Housing Data): {mse:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Notes\n",
        "\n",
        "- `SVR(kernel='rbf')` is often more flexible than linear SVR for capturing nonlinear relationships.\n",
        "- `C` controls the trade-off between smoothness and accuracy.\n",
        "- `epsilon` defines the margin of tolerance where no penalty is given for errors.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GCPLxqYWZ4_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n",
        " Here's a Python program that trains an **SVM classifier with a Polynomial kernel** on a synthetic 2D dataset and visualizes the **decision boundary** using `matplotlib`.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ SVM with Polynomial Kernel + Decision Boundary Visualization\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# üéØ Generate synthetic 2D classification data\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# ‚öñÔ∏è Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# üîß Train SVM with Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, coef0=1)\n",
        "svm_poly.fit(X_scaled, y)\n",
        "\n",
        "# üé® Plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title):\n",
        "    h = 0.02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# üñºÔ∏è Visualize\n",
        "plot_decision_boundary(svm_poly, X_scaled, y, \"SVM with Polynomial Kernel (Degree 3)\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Parameters\n",
        "\n",
        "- `degree=3`: Degree of the polynomial kernel.\n",
        "- `coef0=1`: Controls the influence of higher-order terms.\n",
        "- `C=1.0`: Regularization parameter.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "571O74VAaNDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q25. Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Gaussian Na√Øve Bayes on Breast Cancer Dataset\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train the Gaussian Na√Øve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Na√Øve Bayes classifier: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Output Example\n",
        "You should see something like:\n",
        "```\n",
        "Accuracy of Gaussian Na√Øve Bayes classifier: 0.9474\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Notes\n",
        "- The Breast Cancer dataset is built into `sklearn.datasets`, so no external download is needed.\n",
        "- GaussianNB assumes features follow a normal distribution ‚Äî which works well for this dataset.\n",
        "- You can also explore confusion matrix, precision, recall, and F1-score for deeper evaluation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QYu-HxNea8d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q26. Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20 newsgroups dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## üì∞ Multinomial Na√Øve Bayes on 20 Newsgroups Text Dataset\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset (using a subset for speed)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Step 2: Convert text to feature vectors\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Step 3: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Step 4: Train the Multinomial Na√Øve Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate\n",
        "y_pred = mnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Multinomial Na√Øve Bayes classifier: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Key Points\n",
        "- `MultinomialNB` is ideal for discrete features like word counts.\n",
        "- `CountVectorizer` transforms raw text into a bag-of-words model.\n",
        "- You can expand to all 20 categories or use `TfidfVectorizer` for better feature scaling.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ETQiORBxfDTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q27.Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually.\n",
        " a complete Python program that trains an SVM classifier with different values of the regularization parameter `C`, and visualizes the decision boundaries to show how the margin changes.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Concept Recap**\n",
        "- **SVM (Support Vector Machine)** tries to find the optimal hyperplane that separates classes.\n",
        "- The **C parameter** controls the trade-off between achieving a low error on the training data and maintaining a large margin.\n",
        "  - **Low C** ‚Üí wider margin, more tolerance for misclassification.\n",
        "  - **High C** ‚Üí narrower margin, less tolerance for misclassification.\n",
        "\n",
        "---\n",
        "\n",
        "### üêç **Python Program with Visualization**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load a simple dataset (2 features for easy visualization)\n",
        "X, y = datasets.make_classification(n_samples=100, n_features=2,\n",
        "                                    n_informative=2, n_redundant=0,\n",
        "                                    n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Different values of C to compare\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "# Plotting setup\n",
        "fig, axes = plt.subplots(1, len(C_values), figsize=(20, 4))\n",
        "fig.suptitle(\"SVM Decision Boundaries with Different C Values\", fontsize=16)\n",
        "\n",
        "for ax, C in zip(axes, C_values):\n",
        "    # Train SVM\n",
        "    clf = SVC(kernel='linear', C=C)\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    ax.set_title(f\"C = {C}\")\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=30, edgecolors='k')\n",
        "\n",
        "    # Create grid to evaluate model\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 200),\n",
        "                         np.linspace(ylim[0], ylim[1], 200))\n",
        "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary and margins\n",
        "    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.7,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **What You'll Observe**\n",
        "- **C = 0.01**: Very soft margin, allows more misclassifications.\n",
        "- **C = 100**: Very hard margin, tries to classify all points correctly, possibly overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F1jt7gvYgcaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q28.Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with binary features.\n",
        "Here's a complete Python program to train a **Bernoulli Na√Øve Bayes** classifier for binary classification using a synthetic dataset with binary features.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Concept Recap**\n",
        "- **Bernoulli Na√Øve Bayes** is ideal for binary/boolean features (0 or 1).\n",
        "- It assumes features are independent and follow a Bernoulli distribution.\n",
        "- Commonly used in text classification (e.g., presence/absence of words).\n",
        "\n",
        "---\n",
        "\n",
        "### üêç **Python Program: Bernoulli Na√Øve Bayes on Binary Features**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic binary dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=10, n_redundant=0,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "# Convert features to binary (0 or 1)\n",
        "X_binary = (X > 0).astype(int)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binary, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bernoulli Na√Øve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Output Summary**\n",
        "- **Accuracy**: Shows how well the model performs on unseen data.\n",
        "- **Classification Report**: Includes precision, recall, and F1-score for each class.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SrSGx1WUiRWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q29.Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data.\n",
        " Feature scaling is **crucial** for algorithms like SVM that rely on distance-based calculations. Here's a complete Python program that:\n",
        "\n",
        "1. Trains an SVM on **unscaled** data.\n",
        "2. Trains an SVM on **scaled** data.\n",
        "3. Compares their performance side by side.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Why Feature Scaling Matters for SVM**\n",
        "- SVM uses dot products and distances to find the optimal hyperplane.\n",
        "- Features with larger ranges can dominate the decision boundary.\n",
        "- Scaling ensures all features contribute equally.\n",
        "\n",
        "---\n",
        "\n",
        "### üêç **Python Program: SVM with and without Feature Scaling**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------- Model without Scaling -----------\n",
        "svm_unscaled = SVC(kernel='linear')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# ----------- Model with Scaling -----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_scaled = SVC(kernel='linear')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ----------- Comparison -----------\n",
        "print(\"Accuracy without Scaling:\", round(acc_unscaled, 4))\n",
        "print(\"Accuracy with Scaling   :\", round(acc_scaled, 4))\n",
        "\n",
        "if acc_scaled > acc_unscaled:\n",
        "    print(\"\\n‚úÖ Scaling improved the model performance.\")\n",
        "elif acc_scaled == acc_unscaled:\n",
        "    print(\"\\n‚öñÔ∏è No difference in accuracy, but scaling may still help with convergence.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Unexpected: Unscaled model performed better. Check feature distributions.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Typical Output**\n",
        "```text\n",
        "Accuracy without Scaling: 0.9298\n",
        "Accuracy with Scaling   : 0.9649\n",
        "\n",
        "‚úÖ Scaling improved the model performance.\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WOInzJLJkxk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q30.Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and after Laplace Smoothing.\n",
        " Gaussian Na√Øve Bayes is typically used for **continuous features**, and it models each feature using a **normal distribution**. However, **Laplace smoothing** (also called **additive smoothing**) is generally applied to **categorical features** in models like **Multinomial** or **Bernoulli Na√Øve Bayes**‚Äînot Gaussian.\n",
        "\n",
        "But here's the twist: we can simulate a scenario where Laplace-like smoothing affects the **prior probabilities** or discretized features. So, let‚Äôs do two things:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Plan**\n",
        "1. Train a **Gaussian Na√Øve Bayes** model on continuous data (no smoothing).\n",
        "2. Discretize the features and apply **Multinomial Na√Øve Bayes** with and without Laplace smoothing (`alpha=1` vs `alpha=0`).\n",
        "3. Compare predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### üêç **Python Program: Comparing Predictions With and Without Laplace Smoothing**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------- Gaussian Na√Øve Bayes (no smoothing needed) -----------\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "acc_gnb = accuracy_score(y_test, y_pred_gnb)\n",
        "\n",
        "# ----------- Discretize features for MultinomialNB -----------\n",
        "discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
        "X_train_disc = discretizer.fit_transform(X_train)\n",
        "X_test_disc = discretizer.transform(X_test)\n",
        "\n",
        "# ----------- MultinomialNB without Laplace smoothing (alpha=0) -----------\n",
        "mnb_no_smooth = MultinomialNB(alpha=0.0)\n",
        "mnb_no_smooth.fit(X_train_disc, y_train)\n",
        "y_pred_no_smooth = mnb_no_smooth.predict(X_test_disc)\n",
        "acc_no_smooth = accuracy_score(y_test, y_pred_no_smooth)\n",
        "\n",
        "# ----------- MultinomialNB with Laplace smoothing (alpha=1) -----------\n",
        "mnb_smooth = MultinomialNB(alpha=1.0)\n",
        "mnb_smooth.fit(X_train_disc, y_train)\n",
        "y_pred_smooth = mnb_smooth.predict(X_test_disc)\n",
        "acc_smooth = accuracy_score(y_test, y_pred_smooth)\n",
        "\n",
        "# ----------- Results -----------\n",
        "print(\"GaussianNB Accuracy (continuous features):\", round(acc_gnb, 4))\n",
        "print(\"MultinomialNB Accuracy without smoothing :\", round(acc_no_smooth, 4))\n",
        "print(\"MultinomialNB Accuracy with smoothing    :\", round(acc_smooth, 4))\n",
        "\n",
        "if acc_smooth > acc_no_smooth:\n",
        "    print(\"\\n‚úÖ Laplace smoothing improved MultinomialNB performance.\")\n",
        "elif acc_smooth == acc_no_smooth:\n",
        "    print(\"\\n‚öñÔ∏è No change in accuracy, but smoothing may help with unseen data.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Smoothing reduced accuracy‚Äîpossibly due to overcompensation.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Notes\n",
        "- `alpha=1.0` applies Laplace smoothing.\n",
        "- `alpha=0.0` disables it‚Äîcan cause issues if a feature/class combination is missing.\n",
        "- GaussianNB doesn't use `alpha` because it models continuous distributions.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sp4y5dfwmmta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q31.Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,gamma, kernel)\n",
        "Hyperparameter tuning with `GridSearchCV` is a powerful way to optimize an SVM classifier. Here's a complete Python program that:\n",
        "\n",
        "- Loads a dataset\n",
        "- Trains an SVM classifier\n",
        "- Uses `GridSearchCV` to tune `C`, `gamma`, and `kernel`\n",
        "- Reports the best parameters and model performance\n",
        "\n",
        "---\n",
        "\n",
        "### üîß **SVM Hyperparameters Explained**\n",
        "- `C`: Regularization parameter (higher = less regularization)\n",
        "- `gamma`: Kernel coefficient for ‚Äòrbf‚Äô, ‚Äòpoly‚Äô, and ‚Äòsigmoid‚Äô\n",
        "- `kernel`: Type of kernel function (`linear`, `rbf`, `poly`, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### üêç **Python Program: SVM + GridSearchCV**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# GridSearchCV setup\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"üîç Best Parameters:\", grid_search.best_params_)\n",
        "print(\"‚úÖ Best Cross-Validation Accuracy:\", round(grid_search.best_score_, 4))\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"\\nüìä Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Tips**\n",
        "- You can add `StandardScaler()` before SVM for better performance.\n",
        "- `verbose=1` shows progress during grid search.\n",
        "- Try `RandomizedSearchCV` for faster tuning on large grids.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Ybf7q-IMnkNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q32.Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy.\n",
        "Imbalanced datasets can mislead classifiers like SVM into favoring the majority class. To counter this, we can use **class weighting**, which penalizes misclassification of minority classes more heavily.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è **Why Class Weighting Helps**\n",
        "- Without weighting: SVM may ignore minority class to maximize overall accuracy.\n",
        "- With `class_weight='balanced'`: SVM adjusts the penalty based on class frequencies.\n",
        "\n",
        "---\n",
        "\n",
        "### üêç **Python Program: SVM on Imbalanced Data with Class Weighting**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from collections import Counter\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=2, n_redundant=10,\n",
        "                           n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                           flip_y=0, random_state=42)\n",
        "\n",
        "print(\"Class distribution:\", Counter(y))\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------- SVM without class weighting -----------\n",
        "svm_plain = SVC(kernel='rbf')\n",
        "svm_plain.fit(X_train, y_train)\n",
        "y_pred_plain = svm_plain.predict(X_test)\n",
        "\n",
        "# ----------- SVM with class weighting -----------\n",
        "svm_weighted = SVC(kernel='rbf', class_weight='balanced')\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "\n",
        "# ----------- Evaluation -----------\n",
        "print(\"\\nüîç Without Class Weighting:\")\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_plain), 4))\n",
        "print(classification_report(y_test, y_pred_plain))\n",
        "\n",
        "print(\"\\n‚úÖ With Class Weighting:\")\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_weighted), 4))\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **What to Look For**\n",
        "- **Precision/Recall for minority class (label 1)** should improve with weighting.\n",
        "- Overall accuracy might drop slightly, but **balanced performance** is better.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "MMh7a1waow9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q33.Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data.\n",
        "Na√Øve Bayes is a classic and powerful algorithm for **spam detection**, thanks to its simplicity and effectiveness with text data.\n",
        "\n",
        "Let‚Äôs walk through a complete Python program using **email text data**, applying preprocessing, training a Na√Øve Bayes classifier, and evaluating its performance.\n",
        "\n",
        "---\n",
        "\n",
        "### üì© **Na√Øve Bayes Spam Classifier Using Email Data**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Sample dataset: Replace with your own CSV if needed\n",
        "# Dataset format: two columns ‚Äî 'label' (spam/ham), 'message' (email text)\n",
        "data = pd.read_csv(\"spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
        "data.columns = ['label', 'message']\n",
        "\n",
        "# Convert labels to binary\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Na√Øve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = nb_classifier.predict(X_test_vec)\n",
        "\n",
        "print(\"üìä Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **How It Works**\n",
        "- **CountVectorizer** converts email text into word frequency vectors.\n",
        "- **MultinomialNB** assumes word frequencies follow a multinomial distribution‚Äîperfect for text.\n",
        "- **Stop words** are removed to reduce noise.\n",
        "\n",
        "---\n",
        "\n",
        "### üìÅ Dataset Tip\n",
        "You can use the popular [SMS Spam Collection Dataset](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) from UCI. Just download `spam.csv` and place it in your working directory.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "c3bOGJX6ptp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q34.Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and compare their accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Program: Compare SVM vs Na√Øve Bayes on Text Data**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (SMS Spam Collection from UCI)\n",
        "# Ensure 'spam.csv' is in your working directory\n",
        "data = pd.read_csv(\"spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
        "data.columns = ['label', 'message']\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})  # Convert labels to binary\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# -------- Train Na√Øve Bayes Classifier --------\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_vec, y_train)\n",
        "nb_pred = nb_model.predict(X_test_vec)\n",
        "\n",
        "# -------- Train SVM Classifier --------\n",
        "svm_model = SVC(kernel='linear')  # Linear kernel is best for text\n",
        "svm_model.fit(X_train_vec, y_train)\n",
        "svm_pred = svm_model.predict(X_test_vec)\n",
        "\n",
        "# -------- Compare Accuracy --------\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "\n",
        "print(\"üìä Na√Øve Bayes Accuracy:\", round(nb_accuracy, 4))\n",
        "print(classification_report(y_test, nb_pred, target_names=['Ham', 'Spam']))\n",
        "\n",
        "print(\"\\nüìä SVM Accuracy:\", round(svm_accuracy, 4))\n",
        "print(classification_report(y_test, svm_pred, target_names=['Ham', 'Spam']))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Key Insights**\n",
        "- **Na√Øve Bayes** is fast and surprisingly effective for text.\n",
        "- **SVM** often gives higher precision but can be slower to train.\n",
        "- TF-IDF helps both models by reducing the impact of common words.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O02webU_qHuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q35.Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare results\n",
        " Feature selection can significantly improve model performance by removing irrelevant or noisy features‚Äîespecially useful in text classification where thousands of words may be present.\n",
        "\n",
        "Let‚Äôs walk through a Python program that:\n",
        "\n",
        "1. Loads and vectorizes text data.\n",
        "2. Applies **feature selection** using `SelectKBest` with the **chi-squared test**.\n",
        "3. Trains a **Na√Øve Bayes classifier** with and without feature selection.\n",
        "4. Compares accuracy and classification metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: Feature Selection + Na√Øve Bayes Comparison**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
        "data.columns = ['label', 'message']\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Split data\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train_raw)\n",
        "X_test_vec = vectorizer.transform(X_test_raw)\n",
        "\n",
        "# -------- Without Feature Selection --------\n",
        "nb_full = MultinomialNB()\n",
        "nb_full.fit(X_train_vec, y_train)\n",
        "y_pred_full = nb_full.predict(X_test_vec)\n",
        "\n",
        "# -------- With Feature Selection --------\n",
        "# Select top k features using chi-squared test\n",
        "k = 1000  # You can tune this value\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_sel = selector.fit_transform(X_train_vec, y_train)\n",
        "X_test_sel = selector.transform(X_test_vec)\n",
        "\n",
        "nb_selected = MultinomialNB()\n",
        "nb_selected.fit(X_train_sel, y_train)\n",
        "y_pred_sel = nb_selected.predict(X_test_sel)\n",
        "\n",
        "# -------- Compare Results --------\n",
        "print(\"üìä Without Feature Selection:\")\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_full), 4))\n",
        "print(classification_report(y_test, y_pred_full, target_names=['Ham', 'Spam']))\n",
        "\n",
        "print(\"\\n‚úÖ With Feature Selection (Top\", k, \"features):\")\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_sel), 4))\n",
        "print(classification_report(y_test, y_pred_sel, target_names=['Ham', 'Spam']))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Why Feature Selection Helps**\n",
        "- Reduces dimensionality ‚Üí faster training\n",
        "- Removes noisy or irrelevant words\n",
        "- Improves generalization on unseen data\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqeIP6YPrVyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q36.Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy.\n",
        " The **Wine dataset** is a classic multi-class classification problem, perfect for comparing **One-vs-Rest (OvR)** and **One-vs-One (OvO)** strategies with SVM.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **OvR vs OvO: What's the Difference?**\n",
        "| Strategy | Description | Number of Classifiers |\n",
        "|----------|-------------|------------------------|\n",
        "| OvR      | One classifier per class vs all others | \\( C \\) |\n",
        "| OvO      | One classifier per pair of classes     | \\( \\frac{C(C-1)}{2} \\) |\n",
        "\n",
        "For the Wine dataset with 3 classes:\n",
        "- OvR ‚Üí 3 classifiers\n",
        "- OvO ‚Üí 3 classifiers (since \\( \\frac{3 \\cdot 2}{2} = 3 \\))\n",
        "\n",
        "---\n",
        "\n",
        "### üç∑ **Python Program: SVM with OvR and OvO on Wine Dataset**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# -------- SVM with One-vs-Rest --------\n",
        "svm_ovr = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "\n",
        "# -------- SVM with One-vs-One --------\n",
        "svm_ovo = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "\n",
        "# -------- Compare Accuracy --------\n",
        "print(\"üìä One-vs-Rest Accuracy:\", round(accuracy_score(y_test, y_pred_ovr), 4))\n",
        "print(classification_report(y_test, y_pred_ovr))\n",
        "\n",
        "print(\"\\nüìä One-vs-One Accuracy:\", round(accuracy_score(y_test, y_pred_ovo), 4))\n",
        "print(classification_report(y_test, y_pred_ovo))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **What to Observe**\n",
        "- Accuracy might be similar, but **OvO** can sometimes be more precise for small datasets.\n",
        "- **OvR** is simpler and faster for large class counts.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fANSzhKCsTdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q37.Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy.\n",
        " The **Breast Cancer dataset** is ideal for comparing different SVM kernels:\n",
        "\n",
        "- **Linear kernel**: Best for linearly separable data.\n",
        "- **Polynomial kernel**: Captures complex relationships.\n",
        "- **RBF kernel**: Handles non-linear boundaries using Gaussian similarity.\n",
        "\n",
        "Let‚Äôs train all three and compare their performance.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: SVM with Linear, Polynomial, and RBF Kernels**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# -------- Train SVM with Linear Kernel --------\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# -------- Train SVM with Polynomial Kernel --------\n",
        "svm_poly = SVC(kernel='poly', degree=3)  # Degree can be tuned\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "\n",
        "# -------- Train SVM with RBF Kernel --------\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# -------- Compare Accuracy --------\n",
        "print(\"üìä Linear Kernel Accuracy:\", round(accuracy_score(y_test, y_pred_linear), 4))\n",
        "print(classification_report(y_test, y_pred_linear))\n",
        "\n",
        "print(\"\\nüìä Polynomial Kernel Accuracy:\", round(accuracy_score(y_test, y_pred_poly), 4))\n",
        "print(classification_report(y_test, y_pred_poly))\n",
        "\n",
        "print(\"\\nüìä RBF Kernel Accuracy:\", round(accuracy_score(y_test, y_pred_rbf), 4))\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **What to Observe**\n",
        "| Kernel      | Strengths                          | Weaknesses                        |\n",
        "|-------------|------------------------------------|-----------------------------------|\n",
        "| Linear      | Fast, interpretable                | Limited to linear boundaries      |\n",
        "| Polynomial  | Captures interactions              | Can overfit, slower               |\n",
        "| RBF         | Flexible, handles non-linearity    | Needs tuning (`gamma`, `C`)       |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "E7ENM4I2sybY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q38.Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy.\n",
        "\n",
        " **Stratified K-Fold Cross-Validation** ensures that each fold maintains the same class distribution as the full dataset‚Äîespecially important for imbalanced data.\n",
        "\n",
        "Let‚Äôs use the **Breast Cancer dataset** again and train an **SVM classifier** using stratified K-folds, then compute the **average accuracy** across all folds.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: SVM with Stratified K-Fold Cross-Validation**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Stratified K-Fold setup\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate across folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X_scaled, y), 1):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    model = SVC(kernel='linear')  # You can try 'rbf' or 'poly' too\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"üìÇ Fold {fold} Accuracy: {round(acc, 4)}\")\n",
        "\n",
        "# Average accuracy\n",
        "avg_accuracy = np.mean(accuracies)\n",
        "print(f\"\\n‚úÖ Average Accuracy over {k} folds: {round(avg_accuracy, 4)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Why Stratified K-Fold?**\n",
        "- Preserves class balance in each fold.\n",
        "- Gives a more reliable estimate of model performance.\n",
        "- Reduces variance compared to a single train-test split.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XbQt9tHItJ80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q39.Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare performance.\n",
        " In **Na√Øve Bayes**, prior probabilities represent our belief about class frequencies before seeing any data. By default, these are estimated from the training data, but we can manually set them to test how they affect performance.\n",
        "\n",
        "Let‚Äôs use the **Breast Cancer dataset** and train a **Gaussian Na√Øve Bayes classifier** with:\n",
        "\n",
        "- **Default priors**\n",
        "- **Uniform priors**\n",
        "- **Custom priors** (e.g., skewed toward one class)\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: Na√Øve Bayes with Different Priors**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# -------- Default Prior (learned from data) --------\n",
        "nb_default = GaussianNB()\n",
        "nb_default.fit(X_train, y_train)\n",
        "y_pred_default = nb_default.predict(X_test)\n",
        "\n",
        "# -------- Uniform Prior (equal probability) --------\n",
        "nb_uniform = GaussianNB(priors=[0.5, 0.5])\n",
        "nb_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = nb_uniform.predict(X_test)\n",
        "\n",
        "# -------- Custom Prior (skewed toward class 0) --------\n",
        "nb_custom = GaussianNB(priors=[0.7, 0.3])\n",
        "nb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = nb_custom.predict(X_test)\n",
        "\n",
        "# -------- Compare Results --------\n",
        "print(\"üìä Default Prior Accuracy:\", round(accuracy_score(y_test, y_pred_default), 4))\n",
        "print(classification_report(y_test, y_pred_default))\n",
        "\n",
        "print(\"\\nüìä Uniform Prior Accuracy:\", round(accuracy_score(y_test, y_pred_uniform), 4))\n",
        "print(classification_report(y_test, y_pred_uniform))\n",
        "\n",
        "print(\"\\nüìä Custom Prior Accuracy (0.7/0.3):\", round(accuracy_score(y_test, y_pred_custom), 4))\n",
        "print(classification_report(y_test, y_pred_custom))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **What to Observe**\n",
        "- **Default priors** usually perform best unless you have domain knowledge.\n",
        "- **Uniform priors** assume equal class likelihood‚Äîcan hurt performance if data is imbalanced.\n",
        "- **Custom priors** let you simulate bias or reflect external knowledge.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FpIFZOGnuAlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q40.Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy.\n",
        "**Recursive Feature Elimination (RFE)** is a powerful technique to select the most relevant features by recursively removing the least important ones based on model weights.\n",
        "\n",
        "Let‚Äôs use the **Breast Cancer dataset**, apply RFE with an **SVM classifier**, and compare accuracy:\n",
        "\n",
        "- **Before RFE**: Using all features.\n",
        "- **After RFE**: Using top-k selected features.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: RFE + SVM Classifier Accuracy Comparison**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# -------- SVM without RFE --------\n",
        "svm_full = SVC(kernel='linear')\n",
        "svm_full.fit(X_train, y_train)\n",
        "y_pred_full = svm_full.predict(X_test)\n",
        "\n",
        "# -------- RFE Feature Selection --------\n",
        "k = 10  # Number of top features to select\n",
        "rfe = RFE(estimator=SVC(kernel='linear'), n_features_to_select=k)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Transform data using selected features\n",
        "X_train_rfe = rfe.transform(X_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train SVM on reduced features\n",
        "svm_rfe = SVC(kernel='linear')\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "\n",
        "# -------- Compare Accuracy --------\n",
        "print(\"üìä Accuracy with All Features:\", round(accuracy_score(y_test, y_pred_full), 4))\n",
        "print(classification_report(y_test, y_pred_full))\n",
        "\n",
        "print(f\"\\n‚úÖ Accuracy with Top {k} Features (RFE):\", round(accuracy_score(y_test, y_pred_rfe), 4))\n",
        "print(classification_report(y_test, y_pred_rfe))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Why RFE Helps**\n",
        "- Removes noisy or redundant features.\n",
        "- Improves generalization and reduces overfitting.\n",
        "- Speeds up training and simplifies models.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "EajPEQ3NujBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q41.Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy.\n",
        " Accuracy alone can be misleading‚Äîespecially with imbalanced datasets. Metrics like **Precision**, **Recall**, and **F1-Score** give a much clearer picture of model performance.\n",
        "\n",
        "Let‚Äôs train an **SVM classifier** on the **Breast Cancer dataset** and evaluate it using these metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: SVM Evaluation with Precision, Recall, F1-Score**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Precision, Recall, F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"üîç Evaluation Metrics:\")\n",
        "print(f\"Precision: {round(precision, 4)}\")\n",
        "print(f\"Recall:    {round(recall, 4)}\")\n",
        "print(f\"F1-Score:  {round(f1, 4)}\")\n",
        "\n",
        "# Optional: Full classification report\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìò **Metric Definitions**\n",
        "- **Precision**: How many predicted positives are truly positive.\n",
        "- **Recall**: How many actual positives were correctly predicted.\n",
        "- **F1-Score**: Harmonic mean of precision and recall.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gbSJ7zywvbUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q42.Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss).\n",
        " **Log Loss** (also called **Cross-Entropy Loss**) evaluates the quality of predicted probabilities‚Äînot just hard class labels. It penalizes confident wrong predictions more than uncertain ones.\n",
        "\n",
        "Let‚Äôs train a **Na√Øve Bayes classifier** on the **Breast Cancer dataset** and evaluate it using **log loss**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: Na√Øve Bayes + Log Loss Evaluation**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Na√Øve Bayes Classifier\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Evaluate using Log Loss\n",
        "loss = log_loss(y_test, y_proba)\n",
        "print(f\"üìâ Log Loss (Cross-Entropy): {round(loss, 4)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìò **Why Log Loss Matters**\n",
        "- Measures how close predicted probabilities are to actual labels.\n",
        "- Lower log loss = better calibrated probabilities.\n",
        "- Useful in probabilistic models and decision-making systems.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FsCvZiAlwEkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n",
        " Let's train an **SVM classifier** and visualize its **confusion matrix** using `seaborn`. This gives a clear picture of how well the model distinguishes between classes.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: SVM + Confusion Matrix Visualization**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "model = SVC(kernel='linear', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = data.target_names\n",
        "\n",
        "# Visualize using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"üîç SVM Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Print classification report\n",
        "print(\"\\nüìã Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=labels))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Why This Matters**\n",
        "- **Confusion Matrix** shows true positives, false positives, false negatives, and true negatives.\n",
        "- Helps diagnose class imbalance or misclassification patterns.\n",
        "- `seaborn` makes it visually intuitive.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uSoMVhbnxN4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q44.Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n",
        " While **MSE** penalizes large errors more, **MAE** gives a more balanced view of average prediction error‚Äîespecially useful when outliers shouldn't dominate the evaluation.\n",
        "\n",
        "Let‚Äôs train an **SVM Regressor (SVR)** on the **California Housing dataset** and evaluate it using **Mean Absolute Error (MAE)**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: SVR + MAE Evaluation**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVR model\n",
        "model = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"üìè Mean Absolute Error (MAE): {round(mae, 4)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìò **Why MAE?**\n",
        "- Measures average magnitude of errors in predictions.\n",
        "- Less sensitive to outliers than MSE.\n",
        "- Easier to interpret: ‚ÄúOn average, predictions are off by X units.‚Äù\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "kvScyaKexonN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q45. Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC score.\n",
        " The **ROC-AUC score** is a powerful metric for evaluating binary classifiers‚Äîespecially when class imbalance is a concern. It tells us how well the model separates the positive and negative classes across all thresholds.\n",
        "\n",
        "Let‚Äôs train a **Na√Øve Bayes classifier** and evaluate it using **ROC-AUC**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: Na√Øve Bayes + ROC-AUC Score**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Na√Øve Bayes Classifier\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"üßÆ ROC-AUC Score: {round(roc_auc, 4)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìò **Why ROC-AUC?**\n",
        "- Measures model‚Äôs ability to rank positive instances higher than negative ones.\n",
        "- AUC = 1.0 ‚Üí perfect classifier; AUC = 0.5 ‚Üí no better than random guessing.\n",
        "- Especially useful when classes are imbalanced.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9Sx7ruo-yUl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        " The **Precision-Recall (PR) Curve** is especially insightful when dealing with **imbalanced datasets**‚Äîit focuses on the performance for the positive class.\n",
        "\n",
        "Let‚Äôs train an **SVM classifier** and visualize the **Precision-Recall Curve** using `matplotlib`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Python Program: SVM + Precision-Recall Curve**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with probability estimates\n",
        "model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall pairs\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='darkorange')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"üéØ Precision-Recall Curve (SVM)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìò **Why Precision-Recall Curve?**\n",
        "- Focuses on **positive class performance**.\n",
        "- More informative than ROC when classes are imbalanced.\n",
        "- **Average Precision (AP)** summarizes the curve into a single score.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CigifWb7yzCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with probability estimates\n",
        "model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall pairs\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='darkorange')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"üéØ Precision-Recall Curve (SVM)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "mhzrUWD-zg5i",
        "outputId": "67aba68f-7f10-4f12-a1bb-1a082891a39b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2209453691.py:38: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}